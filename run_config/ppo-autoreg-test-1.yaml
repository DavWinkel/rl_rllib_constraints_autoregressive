cartpole-ppo:
    env: CustomCorrelatedActionsEnv
    run: PPO
    stop:
        episode_reward_mean: 200
        timesteps_total: 1000000
    local_dir: ./.logging
    config:
        # Works for both torch and tf.
        framework: torch
        #num_sgd_iter: 5
        #vf_loss_coeff: 0.0001
        model:
            fcnet_hiddens: [256, 256]
            vf_share_layers: true
            custom_model: autoregressive_model
            custom_action_dist: binary_autoreg_dist
            #use_lstm: True
            #lstm_use_prev_action: True
            #lstm_use_prev_reward: True
            #lstm_cell_size: 256
    verbose: 2

        
